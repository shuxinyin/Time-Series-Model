{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('favorita-grocery-sales-forecasting')\n",
    "# set parent directory as the output path\n",
    "output_path = Path(data_path).parent.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No records will be considered outside these bounds\n",
    "# start_date = datetime(2015, 7, 1)\n",
    "# end_date = datetime(2017, 4, 1)\n",
    "\n",
    "# Where training period ends and the validation period begins\n",
    "# validation_bound = datetime(2016, 7, 1)\n",
    "\n",
    "start_date = datetime(2017, 1, 1)\n",
    "end_date = datetime(2017, 4, 1)\n",
    "\n",
    "validation_bound = datetime(2017, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = 30  # historical scope in time-steps\n",
    "future_len = 10  # futuristic scope in time-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_interval = 3  # time-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables that are known in advance, and will compose the futuristic time-series\n",
    "known_attrs = ['onpromotion',\n",
    "               'day_of_week',\n",
    "               'day_of_month',\n",
    "               'month',\n",
    "               'national_holiday',\n",
    "               'regional_holiday',\n",
    "               'local_holiday',\n",
    "               'open'\n",
    "               ]\n",
    "\n",
    "# The following set of variables will be considered as static, i.e. containing non-temporal information\n",
    "# every attribute which is not listed here will be considered as temporal.\n",
    "static_attrs = ['item_nbr',\n",
    "                'store_nbr',\n",
    "                'city',\n",
    "                'state',\n",
    "                'store_type',\n",
    "                'store_cluster',\n",
    "                'item_family',\n",
    "                'item_class',\n",
    "                'perishable',\n",
    "                ]\n",
    "\n",
    "# The following set of variables will be considered as categorical.\n",
    "# The rest of the variables (which are not listed below) will be considered as numeric.\n",
    "categorical_attrs = ['item_nbr',\n",
    "                     'store_nbr',\n",
    "                     'city',\n",
    "                     'state',\n",
    "                     'store_type',\n",
    "                     'store_cluster',\n",
    "                     'item_family',\n",
    "                     'item_class',\n",
    "                     'perishable',\n",
    "                     'onpromotion',\n",
    "                     'open',\n",
    "                     'day_of_week',\n",
    "                     'month',\n",
    "                     'national_holiday',\n",
    "                     'regional_holiday',\n",
    "                     'local_holiday',\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_signal = 'log_sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will not be included as part of the input data which will end up feeding the model\n",
    "meta_attrs = ['date', 'combination_id', 'temporal_id', 'unit_sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_processed.csv', 'holidays_events.csv', 'items.csv', 'oil.csv', 'sample_submission.csv', 'stores.csv', 'test.csv', 'train.csv', 'transactions.csv']\n"
     ]
    }
   ],
   "source": [
    "file_names = [os.path.basename(f) for f in glob.glob(os.path.join(data_path, '*.{}'.format('csv')))]\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(os.path.join(data_path, 'transactions.csv'), parse_dates=['date'])\n",
    "items_df = pd.read_csv(os.path.join(data_path, 'items.csv'), index_col='item_nbr')\n",
    "oil_df = pd.read_csv(os.path.join(data_path, 'oil.csv'), parse_dates=['date'],index_col='date')\n",
    "holiday_df = pd.read_csv(os.path.join(data_path, 'holidays_events.csv'), parse_dates=['date'],\n",
    "                         dtype={'transferred': bool})\n",
    "stores_df = pd.read_csv(os.path.join(data_path, 'stores.csv'), index_col='store_nbr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9436869, 5)\n"
     ]
    }
   ],
   "source": [
    "list_data_df = []\n",
    "data_chunk_iter = pd.read_csv(os.path.join(data_path, 'train.csv'),\n",
    "                      dtype={'onpromotion': object},\n",
    "                      index_col='id',\n",
    "                      parse_dates=['date'], chunksize=100000)\n",
    "\n",
    "for chunk in data_chunk_iter:\n",
    "    data_df = chunk.loc[(chunk['date'] >= start_date) & (chunk['date'] <= end_date)]\n",
    "    list_data_df.append(data_df)\n",
    "\n",
    "data_df = pd.concat(list_data_df)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101688779</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>99197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101688780</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>103665</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101688781</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101688782</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105857</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101688783</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>106716</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125643</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>2057387</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125644</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>2058758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125645</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>2060793</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125646</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>2061121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125647</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>2061214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9436869 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "id                                                               \n",
       "101688779 2017-01-01         25     99197         1.0       False\n",
       "101688780 2017-01-01         25    103665         7.0       False\n",
       "101688781 2017-01-01         25    105574         1.0       False\n",
       "101688782 2017-01-01         25    105857         4.0       False\n",
       "101688783 2017-01-01         25    106716         2.0       False\n",
       "...              ...        ...       ...         ...         ...\n",
       "111125643 2017-04-01         54   2057387         2.0       False\n",
       "111125644 2017-04-01         54   2058758         1.0       False\n",
       "111125645 2017-04-01         54   2060793         2.0       False\n",
       "111125646 2017-04-01         54   2061121         3.0       False\n",
       "111125647 2017-04-01         54   2061214         1.0       False\n",
       "\n",
       "[9436869 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3370464, 4)\n"
     ]
    }
   ],
   "source": [
    "# we will not use the test data in this demonstration -\n",
    "# the entire dataset will be created using the 'train.csv' file.\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'),\n",
    "                      index_col='id',\n",
    "                      parse_dates=['date'])\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125497040</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>96995</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497041</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>99197</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497042</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>103501</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497043</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>103520</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497044</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>103665</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128867499</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>54</td>\n",
       "      <td>2132163</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128867500</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>54</td>\n",
       "      <td>2132318</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128867501</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>54</td>\n",
       "      <td>2132945</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128867502</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>54</td>\n",
       "      <td>2132957</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128867503</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>54</td>\n",
       "      <td>2134244</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3370464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  store_nbr  item_nbr  onpromotion\n",
       "id                                                    \n",
       "125497040 2017-08-16          1     96995        False\n",
       "125497041 2017-08-16          1     99197        False\n",
       "125497042 2017-08-16          1    103501        False\n",
       "125497043 2017-08-16          1    103520        False\n",
       "125497044 2017-08-16          1    103665        False\n",
       "...              ...        ...       ...          ...\n",
       "128867499 2017-08-31         54   2132163        False\n",
       "128867500 2017-08-31         54   2132318        False\n",
       "128867501 2017-08-31         54   2132945        False\n",
       "128867502 2017-08-31         54   2132957        False\n",
       "128867503 2017-08-31         54   2134244        False\n",
       "\n",
       "[3370464 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ptypes.is_object_dtype(data_df['onpromotion']):\n",
    "    data_df['onpromotion'] = data_df['onpromotion'] == 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.rename(columns={'type': 'store_type', 'cluster': 'store_cluster'}, inplace=True)\n",
    "items_df.rename(columns={'class': 'item_class', 'family': 'item_family'}, inplace=True)\n",
    "oil_df.rename(columns={'dcoilwtico': 'oil_price'}, inplace=True)\n",
    "holiday_df.rename(columns={'type': 'holiday_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lose the null records on the raw dataframe representing oil prices\n",
    "oil_df = oil_df.loc[~oil_df.oil_price.isna()]\n",
    "oil_df = oil_df.resample('1d').ffill().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter, Maniplate & Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have done before\n",
    "# data_df = data_df.loc[(data_df['date'] >= start_date) & (data_df['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_df = data_df.assign(combination_id=data_df['store_nbr'].apply(str) + '_' + data_df['item_nbr'].apply(str))\n",
    "\n",
    "# samples id to avoid big data\n",
    "n=1000\n",
    "combination_id_sample = random.sample(list(data_df[\"combination_id\"].unique()), n)\n",
    "data_df = data_df[data_df[\"combination_id\"].isin(combination_id_sample)] \n",
    "\n",
    "# another index can be used to identify the unique combination of (store,product,date)\n",
    "data_df = data_df.assign(temporal_id=data_df['combination_id'] + '_' + data_df['date'].dt.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>combination_id</th>\n",
       "      <th>temporal_id</th>\n",
       "      <th>open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101688908</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>302952</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>25_302952</td>\n",
       "      <td>25_302952_2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101689194</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>692531</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>25_692531</td>\n",
       "      <td>25_692531_2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101689286</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>838408</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>25_838408</td>\n",
       "      <td>25_838408_2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101689322</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>850333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>25_850333</td>\n",
       "      <td>25_850333_2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101689347</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>875604</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>25_875604</td>\n",
       "      <td>25_875604_2017-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111124903</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>1162382</td>\n",
       "      <td>74.0</td>\n",
       "      <td>True</td>\n",
       "      <td>54_1162382</td>\n",
       "      <td>54_1162382_2017-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111124975</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>1239815</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>54_1239815</td>\n",
       "      <td>54_1239815_2017-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125033</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>1336461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>54_1336461</td>\n",
       "      <td>54_1336461_2017-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125213</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>1463807</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>54_1463807</td>\n",
       "      <td>54_1463807_2017-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111125508</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>54</td>\n",
       "      <td>1997976</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>54_1997976</td>\n",
       "      <td>54_1997976_2017-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61046 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  store_nbr  item_nbr  unit_sales  onpromotion  \\\n",
       "id                                                                   \n",
       "101688908 2017-01-01         25    302952         5.0        False   \n",
       "101689194 2017-01-01         25    692531         3.0        False   \n",
       "101689286 2017-01-01         25    838408         2.0        False   \n",
       "101689322 2017-01-01         25    850333         1.0        False   \n",
       "101689347 2017-01-01         25    875604         1.0        False   \n",
       "...              ...        ...       ...         ...          ...   \n",
       "111124903 2017-04-01         54   1162382        74.0         True   \n",
       "111124975 2017-04-01         54   1239815         9.0        False   \n",
       "111125033 2017-04-01         54   1336461         1.0         True   \n",
       "111125213 2017-04-01         54   1463807         8.0         True   \n",
       "111125508 2017-04-01         54   1997976        10.0        False   \n",
       "\n",
       "          combination_id            temporal_id  open  \n",
       "id                                                     \n",
       "101688908      25_302952   25_302952_2017-01-01     1  \n",
       "101689194      25_692531   25_692531_2017-01-01     1  \n",
       "101689286      25_838408   25_838408_2017-01-01     1  \n",
       "101689322      25_850333   25_850333_2017-01-01     1  \n",
       "101689347      25_875604   25_875604_2017-01-01     1  \n",
       "...                  ...                    ...   ...  \n",
       "111124903     54_1162382  54_1162382_2017-04-01     1  \n",
       "111124975     54_1239815  54_1239815_2017-04-01     1  \n",
       "111125033     54_1336461  54_1336461_2017-04-01     1  \n",
       "111125213     54_1463807  54_1463807_2017-04-01     1  \n",
       "111125508     54_1997976  54_1997976_2017-04-01     1  \n",
       "\n",
       "[61046 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark all the existing records as days in which the relevant stores were open\n",
    "data_df = data_df.assign(open=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal resampling of each combination (1 days interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████▋                                              | 407/1000 [00:02<00:03, 166.24it/s]D:\\miniconda3\\envs\\timeseries\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▌ | 982/1000 [00:06<00:00, 182.55it/s]D:\\miniconda3\\envs\\timeseries\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 161.94it/s]\n"
     ]
    }
   ],
   "source": [
    "sequence_per_combination = []  # a list to contain all the resampled sequences\n",
    "\n",
    "# for each combination\n",
    "for comb_id, comb_df in tqdm(data_df.groupby('combination_id')):\n",
    "    resamp_seq = comb_df.copy()\n",
    "    resamp_seq = resamp_seq.set_index('date').resample('1d').last().reset_index()\n",
    "\n",
    "    resamp_seq['log_sales'] = np.log10(1 + resamp_seq['unit_sales'] + 1e-5)\n",
    "    # newly generated records are assumed to be days in which the store was not open\n",
    "    resamp_seq['open'] = resamp_seq['open'].fillna(0)\n",
    "    # pad with the corresponding information according to the previously available record\n",
    "    for col in ['store_nbr', 'item_nbr', 'onpromotion']:\n",
    "        resamp_seq[col] = resamp_seq[col].fillna(method='ffill')\n",
    "\n",
    "    sequence_per_combination.append(resamp_seq)\n",
    "\n",
    "# combine all the resampled sequences\n",
    "data_df = pd.concat(sequence_per_combination, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['day_of_week'] = pd.to_datetime(data_df['date'].values).dayofweek\n",
    "data_df['day_of_month'] = pd.to_datetime(data_df['date'].values).day\n",
    "data_df['month'] = pd.to_datetime(data_df['date'].values).month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging with other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(stores_df, how='left', on='store_nbr')\n",
    "data_df = data_df.merge(items_df, how='left', on='item_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll ignore holidays that were \"transferred\"\n",
    "holiday_df = holiday_df.loc[~holiday_df.transferred]\n",
    "\n",
    "# National holidays will mark every relevant record (by date)\n",
    "data_df = data_df.assign(national_holiday=data_df.merge(holiday_df.loc[holiday_df.locale == 'National'],\n",
    "                                                        on='date', how='left')['description'].fillna('None')\n",
    "                         )\n",
    "\n",
    "# Regional holidays will mark every relevant record (by date and state)\n",
    "data_df = data_df.assign(regional_holiday=data_df.merge(holiday_df.loc[holiday_df.locale == 'Regional'],\n",
    "                                                        left_on=['date', 'state'],\n",
    "                                                        right_on=['date', 'locale_name'],\n",
    "                                                        how='left'\n",
    "                                                        )['description'].fillna('None')\n",
    "                         )\n",
    "\n",
    "# Local holidays will mark every relevant record (by date and city)\n",
    "data_df = data_df.assign(local_holiday=data_df.merge(holiday_df.loc[holiday_df.locale == 'Local'],\n",
    "                                                     left_on=['date', 'city'],\n",
    "                                                     right_on=['date', 'locale_name'],\n",
    "                                                     how='left'\n",
    "                                                     )['description'].fillna('None')\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(transactions_df, how='left', on=['date', 'store_nbr'])\n",
    "data_df['transactions'] = data_df['transactions'].fillna(-1)\n",
    "\n",
    "data_df = data_df.merge(oil_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = list(data_df.columns)\n",
    "feature_cols = [col for col in all_cols if col not in meta_attrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = {\n",
    "    'static_feats_numeric': [col for col in feature_cols if col in static_attrs and col not in categorical_attrs],\n",
    "    'static_feats_categorical': [col for col in feature_cols if col in static_attrs and col in categorical_attrs],\n",
    "    'historical_ts_numeric': [col for col in feature_cols if col not in static_attrs and col not in categorical_attrs],\n",
    "    'historical_ts_categorical': [col for col in feature_cols if col not in static_attrs and col in categorical_attrs],\n",
    "    'future_ts_numeric': [col for col in feature_cols if col in known_attrs and col not in categorical_attrs],\n",
    "    'future_ts_categorical': [col for col in feature_cols if col in known_attrs and col in categorical_attrs]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate a dictionary to contain the scaler and encoder objects after fitting them\n",
    "scalers = {'numeric': dict(), 'categorical': dict()}\n",
    "# for the categorical variables we would like to keep the cardinalities (how many categories for each variable)\n",
    "categorical_cardinalities = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the the train time range\n",
    "only_train = data_df.loc[data_df['date'] < validation_bound]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the scalers/encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 312.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(feature_cols):\n",
    "    if col in categorical_attrs:\n",
    "        scalers['categorical'][col] = LabelEncoder().fit(only_train[col].values)\n",
    "        categorical_cardinalities[col] = only_train[col].nunique()\n",
    "    else:\n",
    "        if col in ['log_sales']:\n",
    "            scalers['numeric'][col] = StandardScaler().fit(only_train[col].values.astype(float).reshape(-1, 1))\n",
    "        elif col in ['day_of_month']:\n",
    "            scalers['numeric'][col] = MinMaxScaler().fit(only_train[col].values.astype(float).reshape(-1, 1))\n",
    "        else:\n",
    "            scalers['numeric'][col] = QuantileTransformer(n_quantiles=256).fit(\n",
    "                only_train[col].values.astype(float).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform by Applying Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_attrs store_nbr/20\n",
      "categorical_attrs item_nbr/20\n",
      "categorical_attrs onpromotion/20\n",
      "categorical_attrs open/20\n",
      "continous_attrs log_sales/20\n",
      "categorical_attrs day_of_week/20\n",
      "continous_attrs day_of_month/20\n",
      "categorical_attrs month/20\n",
      "categorical_attrs city/20\n",
      "categorical_attrs state/20\n",
      "categorical_attrs store_type/20\n",
      "categorical_attrs store_cluster/20\n",
      "categorical_attrs item_family/20\n",
      "categorical_attrs item_class/20\n",
      "categorical_attrs perishable/20\n",
      "categorical_attrs national_holiday/20\n",
      "categorical_attrs regional_holiday/20\n",
      "categorical_attrs local_holiday/20\n",
      "continous_attrs transactions/20\n",
      "continous_attrs oil_price/20\n"
     ]
    }
   ],
   "source": [
    "for col in feature_cols:\n",
    "    if col in categorical_attrs:\n",
    "        print(f\"categorical_attrs {col}/{len(feature_cols)}\")\n",
    "        le = scalers['categorical'][col]\n",
    "        # handle cases with unseen keys\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        data_df[col] = data_df[col].apply(lambda x: le_dict.get(x, max(le.transform(le.classes_)) + 1))\n",
    "        data_df[col] = data_df[col].astype(np.int32)\n",
    "    else:\n",
    "        print(f\"continous_attrs {col}/{len(feature_cols)}\")\n",
    "        data_df[col] = scalers['numeric'][col].transform(data_df[col].values.reshape(-1, 1)).squeeze()\n",
    "        # data_df[col] = parallel_transform(scalers['numeric'][col], data_df[col])\n",
    "        data_df[col] = data_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['log_sales'].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = {'train': dict(), 'validation': dict(), 'test': dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:24<00:00, 40.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for combination_id, combination_seq in tqdm(data_df.groupby('combination_id')):\n",
    "\n",
    "    # take the complete sequence associated with this combination and break it into the relevant periods\n",
    "    train_subset = combination_seq.loc[combination_seq['date'] < validation_bound]\n",
    "    num_train_records = len(train_subset)\n",
    "    validation_subset_len = num_train_records + future_len\n",
    "    validation_subset = combination_seq.iloc[num_train_records - history_len: validation_subset_len]\n",
    "    test_subset = combination_seq.iloc[validation_subset_len - history_len:]\n",
    "\n",
    "    subsets_dict = {'train': train_subset,\n",
    "                    'validation': validation_subset,\n",
    "                    'test': test_subset}\n",
    "\n",
    "    # for the specific combination we're processing in the current iteration,\n",
    "    # we'd like to go over each subset separately\n",
    "    for subset_key, subset_data in subsets_dict.items():\n",
    "        # sliding window, according to samp_interval skips between adjacent windows\n",
    "        for i in range(0, len(subset_data), samp_interval):\n",
    "            # slice includes history period and horizons period\n",
    "            slc = subset_data.iloc[i: i + history_len + future_len]\n",
    "\n",
    "            if len(slc) < (history_len + future_len):\n",
    "                # skip edge cases, where not enough steps are included\n",
    "                continue\n",
    "\n",
    "            # meta\n",
    "            data_sets[subset_key].setdefault('time_index', []).append(slc.iloc[history_len - 1]['date'])\n",
    "            data_sets[subset_key].setdefault('combination_id', []).append(combination_id)\n",
    "\n",
    "            # static attributes\n",
    "            data_sets[subset_key].setdefault('static_feats_numeric', []).append(\n",
    "                slc.iloc[0][feature_map['static_feats_numeric']].values.astype(np.float32))\n",
    "            data_sets[subset_key].setdefault('static_feats_categorical', []).append(\n",
    "                slc.iloc[0][feature_map['static_feats_categorical']].values.astype(np.int32))\n",
    "\n",
    "            # historical\n",
    "            data_sets[subset_key].setdefault('historical_ts_numeric', []).append(\n",
    "                slc.iloc[:history_len][feature_map['historical_ts_numeric']].values.astype(np.float32).reshape(\n",
    "                    history_len, -1))\n",
    "            data_sets[subset_key].setdefault('historical_ts_categorical', []).append(\n",
    "                slc.iloc[:history_len][feature_map['historical_ts_categorical']].values.astype(np.int32).reshape(\n",
    "                    history_len, -1))\n",
    "\n",
    "            # futuristic (known)\n",
    "            data_sets[subset_key].setdefault('future_ts_numeric', []).append(\n",
    "                slc.iloc[history_len:][feature_map['future_ts_numeric']].values.astype(np.float32).reshape(future_len,\n",
    "                                                                                                           -1))\n",
    "            data_sets[subset_key].setdefault('future_ts_categorical', []).append(\n",
    "                slc.iloc[history_len:][feature_map['future_ts_categorical']].values.astype(np.int32).reshape(future_len,\n",
    "                                                                                                             -1))\n",
    "\n",
    "            # target\n",
    "            data_sets[subset_key].setdefault('target', []).append(\n",
    "                slc.iloc[history_len:]['log_sales'].values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>combination_id</th>\n",
       "      <th>temporal_id</th>\n",
       "      <th>open</th>\n",
       "      <th>log_sales</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>store_type</th>\n",
       "      <th>store_cluster</th>\n",
       "      <th>item_family</th>\n",
       "      <th>item_class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>national_holiday</th>\n",
       "      <th>regional_holiday</th>\n",
       "      <th>local_holiday</th>\n",
       "      <th>transactions</th>\n",
       "      <th>oil_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.358983</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217367</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.161426</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.209804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.692027</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>0.543137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.100654</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.912861</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091373</td>\n",
       "      <td>0.849020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-03-26</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.100654</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135472</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-03-27</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.161426</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110709</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-03-28</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.161426</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-03-30</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-03-30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.579816</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097603</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>9</td>\n",
       "      <td>398</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10_1165987</td>\n",
       "      <td>10_1165987_2017-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.358983</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.287843</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr  item_nbr  unit_sales  onpromotion combination_id  \\\n",
       "0  2017-01-02          9       398         3.0            0     10_1165987   \n",
       "1  2017-01-03          9       398         1.0            0     10_1165987   \n",
       "2  2017-01-04          9       398         2.0            1     10_1165987   \n",
       "3  2017-01-05          9       398         4.0            1     10_1165987   \n",
       "4  2017-01-06          9       398        11.0            1     10_1165987   \n",
       "..        ...        ...       ...         ...          ...            ...   \n",
       "83 2017-03-26          9       398         4.0            0     10_1165987   \n",
       "84 2017-03-27          9       398         1.0            0     10_1165987   \n",
       "85 2017-03-28          9       398         1.0            0     10_1165987   \n",
       "87 2017-03-30          9       398         8.0            0     10_1165987   \n",
       "89 2017-04-01          9       398         3.0            0     10_1165987   \n",
       "\n",
       "              temporal_id  open  log_sales  day_of_week  ...  store_type  \\\n",
       "0   10_1165987_2017-01-02     1  -0.358983            0  ...           2   \n",
       "1   10_1165987_2017-01-03     1  -1.161426            1  ...           2   \n",
       "2   10_1165987_2017-01-04     1  -0.692027            2  ...           2   \n",
       "3   10_1165987_2017-01-05     1  -0.100654            3  ...           2   \n",
       "4   10_1165987_2017-01-06     1   0.912861            4  ...           2   \n",
       "..                    ...   ...        ...          ...  ...         ...   \n",
       "83  10_1165987_2017-03-26     1  -0.100654            6  ...           2   \n",
       "84  10_1165987_2017-03-27     1  -1.161426            0  ...           2   \n",
       "85  10_1165987_2017-03-28     1  -1.161426            1  ...           2   \n",
       "87  10_1165987_2017-03-30     1   0.579816            3  ...           2   \n",
       "89  10_1165987_2017-04-01     1  -0.358983            5  ...           2   \n",
       "\n",
       "    store_cluster  item_family  item_class  perishable  national_holiday  \\\n",
       "0              14           10          13           0                 2   \n",
       "1              14           10          13           0                 1   \n",
       "2              14           10          13           0                 1   \n",
       "3              14           10          13           0                 1   \n",
       "4              14           10          13           0                 1   \n",
       "..            ...          ...         ...         ...               ...   \n",
       "83             14           10          13           0                 1   \n",
       "84             14           10          13           0                 1   \n",
       "85             14           10          13           0                 1   \n",
       "87             14           10          13           0                 1   \n",
       "89             14           10          13           0                 1   \n",
       "\n",
       "    regional_holiday  local_holiday  transactions  oil_price  \n",
       "0                  0              0      0.217367   0.680392  \n",
       "1                  0              0      0.258824   0.209804  \n",
       "2                  0              0      0.113122   0.543137  \n",
       "3                  0              0      0.117647   0.696078  \n",
       "4                  0              0      0.091373   0.849020  \n",
       "..               ...            ...           ...        ...  \n",
       "83                 0              0      0.135472   0.000000  \n",
       "84                 0              0      0.110709   0.000000  \n",
       "85                 0              0      0.088235   0.000000  \n",
       "87                 0              0      0.097603   0.000000  \n",
       "89                 0              0      0.287843   0.000000  \n",
       "\n",
       "[73 rows x 24 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df[\"combination_id\"]=='10_1165987']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each set\n",
    "for set_key in list(data_sets.keys()):\n",
    "    # for each component in the set\n",
    "    for arr_key in list(data_sets[set_key].keys()):\n",
    "        # list of arrays will be concatenated\n",
    "        if isinstance(data_sets[set_key][arr_key], np.ndarray):\n",
    "            data_sets[set_key][arr_key] = np.stack(data_sets[set_key][arr_key], axis=0)\n",
    "        # lists will be transformed into arrays\n",
    "        else:\n",
    "            data_sets[set_key][arr_key] = np.array(data_sets[set_key][arr_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'data.pickle'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'data_sets': data_sets,\n",
    "        'feature_map': feature_map,\n",
    "        'scalers': scalers,\n",
    "        'categorical_cardinalities': categorical_cardinalities\n",
    "    }, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.614px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
