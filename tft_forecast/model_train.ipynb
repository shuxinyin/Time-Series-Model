{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9516176-453d-4878-8358-dca227a73b48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tft_torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01minit\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, Subset\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtft_torch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemporalFusionTransformer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtft_torch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtft_loss\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tft_torch'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from typing import Dict,List,Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf,DictConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tft_torch.tft import TemporalFusionTransformer\n",
    "import tft_torch.loss as tft_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e072b6-2eb6-4cbd-98ad-e1516b3e98f2",
   "metadata": {},
   "source": [
    "### Data-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b309a5-65e0-4a64-bfdc-d995b4cf7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '.../data/favorita/data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bc8d0-86a2-4cad-9727-d4af62271974",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path,'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb247069-5e80-4e7d-b711-9f6085ca0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd73ba-630e-42b9-bcea-3b57f87f4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_name in data['data_sets']:\n",
    "    print('=======')\n",
    "    print(set_name)\n",
    "    print('=======')\n",
    "    for arr_name,arr in data['data_sets'][set_name].items():\n",
    "        print(f\"{arr_name} (shape,dtype)\")\n",
    "        print(arr.shape, arr.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103f9a6-c089-4474-90e3-27d74a314e99",
   "metadata": {},
   "source": [
    "### Modeling configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bcacd-d302-4a2f-983a-f5d07d70dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {'optimization':\n",
    "                 {\n",
    "                     'batch_size': {'training': 256, 'inference': 4096},\n",
    "                     'learning_rate': 0.001,\n",
    "                     'max_grad_norm': 1.0,\n",
    "                 }\n",
    "                 ,\n",
    "                 'model':\n",
    "                 {\n",
    "                     'dropout': 0.05,\n",
    "                     'state_size': 64,\n",
    "                     'output_quantiles': [0.1, 0.5, 0.9],\n",
    "                     'lstm_layers': 2,\n",
    "                     'attention_heads': 4\n",
    "                 },\n",
    "                 # these arguments are related to possible extensions of the model class\n",
    "                 'task_type':'regression',\n",
    "                 'target_window_start': None\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9201b85-d65f-4a7e-850e-2f88a11918e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = data['feature_map']\n",
    "cardinalities_map = data['categorical_cardinalities']\n",
    "\n",
    "structure = {\n",
    "    'num_historical_numeric': len(feature_map['historical_ts_numeric']),\n",
    "    'num_historical_categorical': len(feature_map['historical_ts_categorical']),\n",
    "    'num_static_numeric': len(feature_map['static_feats_numeric']),\n",
    "    'num_static_categorical': len(feature_map['static_feats_categorical']),\n",
    "    'num_future_numeric': len(feature_map['future_ts_numeric']),\n",
    "    'num_future_categorical': len(feature_map['future_ts_categorical']),\n",
    "    'historical_categorical_cardinalities': [cardinalities_map[feat] + 1 for feat in feature_map['historical_ts_categorical']],\n",
    "    'static_categorical_cardinalities': [cardinalities_map[feat] + 1 for feat in feature_map['static_feats_categorical']],\n",
    "    'future_categorical_cardinalities': [cardinalities_map[feat] + 1 for feat in feature_map['future_ts_categorical']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12976fb-8638-46de-ae28-6da782f4ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration['data_props'] = structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ce0f0-d01a-4154-88a5-8e2700ca2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config=OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894eb27-09a0-4159-aea6-2bef6ac28de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "        for names in m._all_weights:\n",
    "            for name in filter(lambda n: \"bias\" in n, names):\n",
    "                bias = getattr(m, name)\n",
    "                n = bias.size(0)\n",
    "                bias.data[:n // 3].fill_(-1.)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e422f6-b510-49da-a50d-4cb3a9fe8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ffacb-c2e3-40a4-b635-eabda17614ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969d227-769c-44e9-80c5-5f68871090b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(filter(lambda p: p.requires_grad, list(model.parameters())),\n",
    "                lr=configuration['optimization']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f82e83-df65-4752-8afa-b99aa00bae9c",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcee06-29ca-4f30-99cc-7a7302d3ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDataSet(Dataset):\n",
    "    def __init__(self, array_dict: Dict[str, np.ndarray]):\n",
    "        self.keys_list = []\n",
    "        for k, v in array_dict.items():\n",
    "            self.keys_list.append(k)\n",
    "            if np.issubdtype(v.dtype, np.dtype('bool')):\n",
    "                setattr(self, k, torch.ByteTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int8):\n",
    "                setattr(self, k, torch.CharTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int16):\n",
    "                setattr(self, k, torch.ShortTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int32):\n",
    "                setattr(self, k, torch.IntTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int64):\n",
    "                setattr(self, k, torch.LongTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.float32):\n",
    "                setattr(self, k, torch.FloatTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.float64):\n",
    "                setattr(self, k, torch.DoubleTensor(v))\n",
    "            else:\n",
    "                setattr(self, k, torch.FloatTensor(v))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {k: getattr(self, k)[index] for k in self.keys_list}\n",
    "\n",
    "    def __len__(self):\n",
    "        return getattr(self, self.keys_list[0]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ca3e1-cc2a-40f0-9fb8-8669b44d93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "def get_set_and_loaders(data_dict: Dict[str, np.ndarray],\n",
    "                        shuffled_loader_config: Dict,\n",
    "                        serial_loader_config: Dict,\n",
    "                        ignore_keys: List[str] = None,\n",
    "                        ) -> Tuple[torch.utils.data.Dataset, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    dataset = DictDataSet({k:v for k,v in data_dict.items() if (ignore_keys and k not in ignore_keys)})\n",
    "    loader = torch.utils.data.DataLoader(dataset,**shuffled_loader_config)\n",
    "    serial_loader = torch.utils.data.DataLoader(dataset,**serial_loader_config)\n",
    "\n",
    "    return dataset,iter(recycle(loader)),serial_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55298fb4-c8da-4568-ab58-ac9d7bccc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_loader_config = {'batch_size': configuration['optimization']['batch_size']['training'],\n",
    "                'drop_last': True,\n",
    "                'shuffle':True}\n",
    "\n",
    "serial_loader_config = {'batch_size': configuration['optimization']['batch_size']['inference'],\n",
    "                'drop_last': False,\n",
    "                'shuffle':False}\n",
    "\n",
    "# the following fields do not contain actual data, but are only identifiers of each observation\n",
    "meta_keys = ['time_index','combination_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77d0b6-bf34-45bf-9230-bf37fb90e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,train_loader,train_serial_loader = get_set_and_loaders(data['data_sets']['train'],\n",
    "                                                                shuffled_loader_config,\n",
    "                                                                serial_loader_config,\n",
    "                                                                ignore_keys=meta_keys)\n",
    "validation_set,validation_loader,validation_serial_loader = get_set_and_loaders(data['data_sets']['validation'],\n",
    "                                                                shuffled_loader_config,\n",
    "                                                                serial_loader_config,\n",
    "                                                                ignore_keys=meta_keys)\n",
    "test_set,test_loader,test_serial_loader = get_set_and_loaders(data['data_sets']['test'],\n",
    "                                                                shuffled_loader_config,\n",
    "                                                                serial_loader_config,\n",
    "                                                                ignore_keys=meta_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58203e9-c95d-436f-832a-e56c85d0f1d7",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75810e-347a-4048-ae8c-b029531dcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueAggregator(object):\n",
    "    def __init__(self, max_size):\n",
    "        self._queued_list = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def append(self, elem):\n",
    "        self._queued_list.append(elem)\n",
    "        if len(self._queued_list) > self.max_size:\n",
    "            self._queued_list.pop(0)\n",
    "\n",
    "    def get(self):\n",
    "        return self._queued_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babed740-6330-4ee5-b9ec-319114357dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e545e-4c36-4305-929c-6bd98b4f4b8f",
   "metadata": {},
   "source": [
    "### Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031ec68-10c2-4b42-a9ef-84a57b19d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If early stopping is not triggered, after how many epochs should we quit training\n",
    "max_epochs = 10000\n",
    "# how many training batches will compose a single training epoch\n",
    "epoch_iters = 200\n",
    "# upon completing a training epoch, we perform an evaluation of all the subsets\n",
    "# eval_iters will define how many batches of each set will compose a single evaluation round\n",
    "eval_iters = 500\n",
    "# during training, on what frequency should we display the monitored performance\n",
    "log_interval = 20\n",
    "# what is the running-window used by our QueueAggregator object for monitoring the training performance\n",
    "ma_queue_size = 50\n",
    "# how many evaluation rounds should we allow,\n",
    "# without any improvement in the performance observed on the validation set\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5b97e-211d-40b8-bf3b-4034183241d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize early stopping mechanism\n",
    "es = EarlyStopping(patience=patience)\n",
    "# initialize the loss aggregator for running window performance estimation\n",
    "loss_aggregator = QueueAggregator(max_size=ma_queue_size)\n",
    "\n",
    "# initialize counters\n",
    "batch_idx = 0\n",
    "epoch_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3456cf5-0e60-47e7-90c0-700a3362e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_tensor = torch.tensor(configuration['model']['output_quantiles']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f65971-a7e2-4abc-a973-0838f73a62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch: Dict[str,torch.tensor],\n",
    "                  model: nn.Module,\n",
    "                  quantiles_tensor: torch.tensor,\n",
    "                  device:torch.device):\n",
    "    if is_cuda:\n",
    "        for k in list(batch.keys()):\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    batch_outputs = model(batch)\n",
    "    labels = batch['target']\n",
    "\n",
    "    predicted_quantiles = batch_outputs['predicted_quantiles']\n",
    "    q_loss, q_risk, _ = tft_loss.get_quantiles_loss_and_q_risk(outputs=predicted_quantiles,\n",
    "                                                              targets=labels,\n",
    "                                                              desired_quantiles=quantiles_tensor)\n",
    "    return q_loss, q_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab1662-e2b1-401d-bff2-e4f7eb663bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch_idx < max_epochs:\n",
    "    print(f\"Starting Epoch Index {epoch_idx}\")\n",
    "\n",
    "    # evaluation round\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # for each subset\n",
    "        for subset_name, subset_loader in zip(['train','validation','test'],[train_loader,validation_loader,test_loader]):\n",
    "            print(f\"Evaluating {subset_name} set\")\n",
    "\n",
    "            q_loss_vals, q_risk_vals = [],[] # used for aggregating performance along the evaluation round\n",
    "            for _ in range(eval_iters):\n",
    "                # get batch\n",
    "                batch = next(subset_loader)\n",
    "                # process batch\n",
    "                batch_loss,batch_q_risk = process_batch(batch=batch,model=model,quantiles_tensor=quantiles_tensor,device=device)\n",
    "                # accumulate performance\n",
    "                q_loss_vals.append(batch_loss)\n",
    "                q_risk_vals.append(batch_q_risk)\n",
    "\n",
    "            # aggregate and average\n",
    "            eval_loss = torch.stack(q_loss_vals).mean(axis=0)\n",
    "            eval_q_risk = torch.stack(q_risk_vals,axis=0).mean(axis=0)\n",
    "\n",
    "            # keep for feeding the early stopping mechanism\n",
    "            if subset_name == 'validation':\n",
    "                validation_loss = eval_loss\n",
    "\n",
    "            # log performance\n",
    "            print(f\"Epoch: {epoch_idx}, Batch Index: {batch_idx}\" + \\\n",
    "                  f\"- Eval {subset_name} - \" + \\\n",
    "                  f\"q_loss = {eval_loss:.5f} , \" + \\\n",
    "                  \" , \".join([f\"q_risk_{q:.1} = {risk:.5f}\" for q,risk in zip(quantiles_tensor,eval_q_risk)]))\n",
    "\n",
    "    # switch to training mode\n",
    "    model.train()\n",
    "\n",
    "    # update early stopping mechanism and stop if triggered\n",
    "    if es.step(validation_loss):\n",
    "        print('Performing early stopping...!')\n",
    "        break\n",
    "\n",
    "    # initiating a training round\n",
    "    for _ in range(epoch_iters):\n",
    "        # get training batch\n",
    "        batch = next(train_loader)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # process batch\n",
    "        loss,_ = process_batch(batch=batch,\n",
    "                              model=model,\n",
    "                              quantiles_tensor=quantiles_tensor,\n",
    "                              device=device)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        if configuration['optimization']['max_grad_norm'] > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), configuration['optimization']['max_grad_norm'])\n",
    "        # update weights\n",
    "        opt.step()\n",
    "\n",
    "        # accumulate performance\n",
    "        loss_aggregator.append(loss.item())\n",
    "\n",
    "        # log performance\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch_idx}, Batch Index: {batch_idx} - Train Loss = {np.mean(loss_aggregator.get())}\")\n",
    "\n",
    "        # completed batch\n",
    "        batch_idx += 1\n",
    "\n",
    "    # completed epoch\n",
    "    epoch_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
