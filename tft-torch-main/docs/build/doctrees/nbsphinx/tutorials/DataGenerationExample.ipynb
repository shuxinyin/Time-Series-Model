{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289225c5",
   "metadata": {},
   "source": [
    "# Favorita Dataset Creation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cede6",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the creation of a dataset in a format that can be fed to the `TemporalFusionTransformer`.<br/>\n",
    "It does not demonstrate direct usage of the implementation suggested in this package, but merely suggesting a methodology for generating a suitable dataset.<br/>\n",
    "\n",
    "### Data-related notes\n",
    "* The dataset used for this demonstration is the [__*Corporaci√≥n Favorita Grocery Sales Forecasting*__](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/overview) dataset, hosted on [Kaggle](https://www.kaggle.com/). One can use [Kaggle API](https://github.com/Kaggle/kaggle-api) in order to download the dataset.\n",
    "* The set of relevant CSV files are download as one compressed zip archive. Unarchiving the zip file will generate a set of **.7z** compressed files, which require unarchiving as well - ending with the set CSV files required for this demonstration.\n",
    "* The following procedure was inspired by the one implemented on [*google-research* repository](https://github.com/google-research/google-research), which can be found [here](https://github.com/google-research/google-research/tree/master/tft).\n",
    "\n",
    "<ins>__*Note*__:</ins> the presented implementation might not be the most efficient way for achieving the processed dataset, and the steps demonstrated below are elaborated for clarity. Moreover, the time boundaries settings can be adapted and modified for using a shorter period of time, as processing the entire dataset might take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5dc46e",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d39412",
   "metadata": {},
   "source": [
    "## Configuration details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7443424",
   "metadata": {},
   "source": [
    "Set the path to the directory containing the CSV files, as well as the output path, where the processed dataset will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath('.../data/favorita/raw')\n",
    "# set parent directory as the output path\n",
    "output_path = Path(data_path).parent.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94731506",
   "metadata": {},
   "source": [
    "Set the time boundaries according to which the data will be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e58f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No records will be considered outside these bounds\n",
    "start_date = datetime(2013, 7, 1)\n",
    "end_date = datetime(2017, 4, 1)\n",
    "\n",
    "# Where training period ends and the validation period begins\n",
    "validation_bound = datetime(2016, 7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17ce2e",
   "metadata": {},
   "source": [
    "We also need to set what is the historical scope, in terms of temporal steps, to conisder for each observation, as well as the maximal horizon (in time-steps) for which the prediction will be required, for each observation. In our case, each observation corresponds to a time-series, and the time-steps correspond to days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = 90  # historical scope in time-steps\n",
    "future_len = 30  # futuristic scope in time-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0bc44",
   "metadata": {},
   "source": [
    "One more temporal configuration argument we need to set is the sampling interval; In order ease the processing of the dataset, we generate a new shiny time-series, spaced with ```samp_interval``` steps from the adjacent time-series (imposing some overlap between adjacent observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09090055",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_interval = 5  # time-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7f329",
   "metadata": {},
   "source": [
    "### Attributes configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784abea",
   "metadata": {},
   "source": [
    "For the multi-horizon forecasting scenario, we consider three primary channels of information flowing into the model as input data:\n",
    "* past/historical temporal information, which is the observed time-series.\n",
    "* static information, in the form of non-temporal attributes associated with the observation.\n",
    "* futuristic temporal information, which is known in advance, for each of the horizons we are about the predict.\n",
    "\n",
    "<br/>\n",
    "Each of this channels can be composed of numeric variables, and from categorical variables.\n",
    "In the following sections, one can find the specification of each attribute's assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d414ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables that are known in advance, and will compose the futuristic time-series\n",
    "known_attrs = ['onpromotion',\n",
    "               'day_of_week',\n",
    "               'day_of_month',\n",
    "               'month',\n",
    "               'national_holiday',\n",
    "               'regional_holiday',\n",
    "               'local_holiday',\n",
    "               'open'\n",
    "               ]\n",
    "\n",
    "# The following set of variables will be considered as static, i.e. containing non-temporal information\n",
    "# every attribute which is not listed here will be considered as temporal.\n",
    "static_attrs = ['item_nbr',\n",
    "                'store_nbr',\n",
    "                'city',\n",
    "                'state',\n",
    "                'store_type',\n",
    "                'store_cluster',\n",
    "                'item_family',\n",
    "                'item_class',\n",
    "                'perishable',\n",
    "                ]\n",
    "\n",
    "# The following set of variables will be considered as categorical.\n",
    "# The rest of the variables (which are not listed below) will be considered as numeric.\n",
    "categorical_attrs = ['item_nbr',\n",
    "                     'store_nbr',\n",
    "                     'city',\n",
    "                     'state',\n",
    "                     'store_type',\n",
    "                     'store_cluster',\n",
    "                     'item_family',\n",
    "                     'item_class',\n",
    "                     'perishable',\n",
    "                     'onpromotion',\n",
    "                     'open',\n",
    "                     'day_of_week',\n",
    "                     'month',\n",
    "                     'national_holiday',\n",
    "                     'regional_holiday',\n",
    "                     'local_holiday',\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7aefd5",
   "metadata": {},
   "source": [
    "We also need to specify which of the attributes represents the signal we would like to predict into the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_signal = 'log_sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866a676",
   "metadata": {},
   "source": [
    "and a list of variables which are not to be considered as actual features - these can be the time index associated with each record, the ID associated with the observation, or features that are already represented by some other variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df175a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will not be included as part of the input data which will end up feeding the model\n",
    "meta_attrs = ['date', 'combination_id', 'temporal_id', 'unit_sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21d31a",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7fcf8",
   "metadata": {},
   "source": [
    "Listing the relevant files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf785970",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [os.path.basename(f) for f in glob.glob(os.path.join(data_path, '*.{}'.format('csv')))]\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43450c3e",
   "metadata": {},
   "source": [
    "Load the CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(os.path.join(data_path, 'transactions.csv'), parse_dates=['date'],\n",
    "                              infer_datetime_format=True)\n",
    "items_df = pd.read_csv(os.path.join(data_path, 'items.csv'), index_col='item_nbr')\n",
    "oil_df = pd.read_csv(os.path.join(data_path, 'oil.csv'), parse_dates=['date'], infer_datetime_format=True,\n",
    "                     index_col='date')\n",
    "holiday_df = pd.read_csv(os.path.join(data_path, 'holidays_events.csv'), parse_dates=['date'],\n",
    "                         infer_datetime_format=True,\n",
    "                         dtype={'transferred': bool})\n",
    "stores_df = pd.read_csv(os.path.join(data_path, 'stores.csv'), index_col='store_nbr')\n",
    "\n",
    "data_df = pd.read_csv(os.path.join(data_path, 'train.csv'),\n",
    "                      dtype={'onpromotion': object},\n",
    "                      index_col='id',\n",
    "                      parse_dates=['date'], infer_datetime_format=True)\n",
    "# we will not use the test data in this demonstration - \n",
    "# the entire dataset will be created using the 'train.csv' file.\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'),\n",
    "                      index_col='id',\n",
    "                      parse_dates=['date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb6069",
   "metadata": {},
   "source": [
    "and fix nulls on the ```onpromotion``` indicator, transforming this attribute to ```bool``` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89585fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ptypes.is_object_dtype(data_df['onpromotion']):\n",
    "    data_df['onpromotion'] = data_df['onpromotion'] == 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63920d2c",
   "metadata": {},
   "source": [
    "Some of the columns are renamed for better clarity when the information will be gathered from the various sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.rename(columns={'type': 'store_type', 'cluster': 'store_cluster'}, inplace=True)\n",
    "items_df.rename(columns={'class': 'item_class', 'family': 'item_family'}, inplace=True)\n",
    "oil_df.rename(columns={'dcoilwtico': 'oil_price'}, inplace=True)\n",
    "holiday_df.rename(columns={'type': 'holiday_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2c6a5",
   "metadata": {},
   "source": [
    "And the oil price is interpolated (```method='ffill'```) before we associate it with the other temporal records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ada56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lose the null records on the raw dataframe representing oil prices\n",
    "oil_df = oil_df.loc[~oil_df.oil_price.isna()]\n",
    "oil_df = oil_df.resample('1d').ffill().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c104585",
   "metadata": {},
   "source": [
    "## Filter, Maniplate & Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d186c",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a6441",
   "metadata": {},
   "source": [
    "Before merging and joining the other sources of data into the ```data_df```, which represents the primary source of data associated with each observation, we restrict it and filter in order to keep only the records within the boundaries we set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.loc[(data_df['date'] >= start_date) & (data_df['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f6ce0",
   "metadata": {},
   "source": [
    "### Manipulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5811ad7",
   "metadata": {},
   "source": [
    "In the dataset we're dealing with, each time-series is associated with two primary entities:\n",
    "* the selling store\n",
    "* the sold product\n",
    "\n",
    "Hence, the following snippet will generate an ID, ```combination_id```, which will identify the combination of a specific store and a specific product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.assign(combination_id=data_df['store_nbr'].apply(str) + '_' + data_df['item_nbr'].apply(str))\n",
    "# another index can be used to identify the unique combination of (store,product,date)\n",
    "data_df = data_df.assign(temporal_id=data_df['combination_id'] + '_' + data_df['date'].dt.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6458b7a",
   "metadata": {},
   "source": [
    "In addition, we discard (store,item) combinations with negative sales observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each combination, we calculate the minimal unit_sales value\n",
    "min_sales = data_df.groupby('combination_id', as_index=False)['unit_sales'].min()\n",
    "# keep only combination with non-negative sales.\n",
    "data_df = data_df.loc[data_df['combination_id'].isin(min_sales.loc[min_sales.unit_sales >= 0, 'combination_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f6c1c",
   "metadata": {},
   "source": [
    "And mark all the existing records as days in which the relevant stores were open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark all the existing records as days in which the relevant stores were open\n",
    "data_df = data_df.assign(open=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6046d",
   "metadata": {},
   "source": [
    "### Temporal resampling of each combination (1 days interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d95f1",
   "metadata": {},
   "source": [
    "We are generating a dense sequence of records for each combination by resamplings, so that for each step in the time-window covered by this sequence, there will be a corresponding records.\n",
    "The records generated by this resampling procedure, will be considered as days where the specific store was closed (```open=False```).<br/>\n",
    "__*Note*__: As part of the resampling, we also assign a new column to contain ```log_sales``` which will be our target signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc67976",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_per_combination = []  # a list to contain all the resampled sequences\n",
    "\n",
    "# for each combination\n",
    "for comb_id, comb_df in tqdm(data_df.groupby('combination_id')):\n",
    "    resamp_seq = comb_df.copy()\n",
    "    resamp_seq = resamp_seq.set_index('date').resample('1d').last().reset_index()\n",
    "\n",
    "    resamp_seq['log_sales'] = np.log10(1 + resamp_seq['unit_sales'])\n",
    "    # newly generated records are assumed to be days in which the store was not open\n",
    "    resamp_seq['open'] = resamp_seq['open'].fillna(0)\n",
    "    # pad with the corresponding information according to the previously available record\n",
    "    for col in ['store_nbr', 'item_nbr', 'onpromotion']:\n",
    "        resamp_seq[col] = resamp_seq[col].fillna(method='ffill')\n",
    "\n",
    "    sequence_per_combination.append(resamp_seq)\n",
    "\n",
    "# combine all the resampled sequences\n",
    "data_df = pd.concat(sequence_per_combination, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb833df5",
   "metadata": {},
   "source": [
    "## Gathering Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655ff02",
   "metadata": {},
   "source": [
    "Before merging the other sources, we can add some time-related information using the specified date associated with each record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['day_of_week'] = pd.to_datetime(data_df['date'].values).dayofweek\n",
    "data_df['day_of_month'] = pd.to_datetime(data_df['date'].values).day\n",
    "data_df['month'] = pd.to_datetime(data_df['date'].values).month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5cd95",
   "metadata": {},
   "source": [
    "### Merging with other sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9c299",
   "metadata": {},
   "source": [
    "Adding the metadata associated with each store and item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(stores_df, how='left', on='store_nbr')\n",
    "data_df = data_df.merge(items_df, how='left', on='item_nbr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371952d7",
   "metadata": {},
   "source": [
    "Adding the holiday-related information associated with each date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll ignore holidays that were \"transferred\"\n",
    "holiday_df = holiday_df.loc[~holiday_df.transferred]\n",
    "\n",
    "# National holidays will mark every relevant record (by date)\n",
    "data_df = data_df.assign(national_holiday=data_df.merge(holiday_df.loc[holiday_df.locale == 'National'],\n",
    "                                                        on='date', how='left')['description'].fillna('None')\n",
    "                         )\n",
    "\n",
    "# Regional holidays will mark every relevant record (by date and state)\n",
    "data_df = data_df.assign(regional_holiday=data_df.merge(holiday_df.loc[holiday_df.locale == 'Regional'],\n",
    "                                                        left_on=['date', 'state'],\n",
    "                                                        right_on=['date', 'locale_name'],\n",
    "                                                        how='left'\n",
    "                                                        )['description'].fillna('None')\n",
    "                         )\n",
    "\n",
    "# Local holidays will mark every relevant record (by date and city)\n",
    "data_df = data_df.assign(local_holiday=data_df.merge(holiday_df.loc[holiday_df.locale == 'Local'],\n",
    "                                                     left_on=['date', 'city'],\n",
    "                                                     right_on=['date', 'locale_name'],\n",
    "                                                     how='left'\n",
    "                                                     )['description'].fillna('None')\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33af7c2",
   "metadata": {},
   "source": [
    "Finally, we're merging the transactions data, as well as the oil price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1253d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(transactions_df, how='left', on=['date', 'store_nbr'])\n",
    "data_df['transactions'] = data_df['transactions'].fillna(-1)\n",
    "\n",
    "data_df = data_df.merge(oil_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138f1bf",
   "metadata": {},
   "source": [
    "### Inferring Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94ad50",
   "metadata": {},
   "source": [
    "Now that the entire dataset is composed, we'll use the attributes-related configuration we've set above. The complete feature set is retrieved by filtering out the ```meta_attrs``` from the columns list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = list(data_df.columns)\n",
    "feature_cols = [col for col in all_cols if col not in meta_attrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476bb6c",
   "metadata": {},
   "source": [
    "Then, we'll create the list of attributes for each channel of input.\n",
    "We'll need such list for each combination of *(static/historical/futuristic)* and *(numeric/categorical)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = {\n",
    "    'static_feats_numeric': [col for col in feature_cols if col in static_attrs and col not in categorical_attrs],\n",
    "    'static_feats_categorical': [col for col in feature_cols if col in static_attrs and col in categorical_attrs],\n",
    "    'historical_ts_numeric': [col for col in feature_cols if col not in static_attrs and col not in categorical_attrs],\n",
    "    'historical_ts_categorical': [col for col in feature_cols if col not in static_attrs and col in categorical_attrs],\n",
    "    'future_ts_numeric': [col for col in feature_cols if col in known_attrs and col not in categorical_attrs],\n",
    "    'future_ts_categorical': [col for col in feature_cols if col in known_attrs and col in categorical_attrs]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66827213",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67dbf9",
   "metadata": {},
   "source": [
    "We would like all of the input variables fed to the model to have similar scales.\n",
    "Hence, each variable will be scaled (if it is numeric) or encoded (in case it is categorical).<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate a dictionary to contain the scaler and encoder objects after fitting them\n",
    "scalers = {'numeric': dict(), 'categorical': dict()}\n",
    "# for the categorical variables we would like to keep the cardinalities (how many categories for each variable)\n",
    "categorical_cardinalities = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b662f63",
   "metadata": {},
   "source": [
    "The scalers/encoders are fit according to the training set/period.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the the train time range\n",
    "only_train = data_df.loc[data_df['date'] < validation_bound]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4856fd6",
   "metadata": {},
   "source": [
    "*Note*: The specific scaling method for each numeric variable was selected after examining its distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a12ff",
   "metadata": {},
   "source": [
    "### Fitting the scalers/encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e706a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(feature_cols):\n",
    "    if col in categorical_attrs:\n",
    "        scalers['categorical'][col] = LabelEncoder().fit(only_train[col].values)\n",
    "        categorical_cardinalities[col] = only_train[col].nunique()\n",
    "    else:\n",
    "        if col in ['log_sales']:\n",
    "            scalers['numeric'][col] = StandardScaler().fit(only_train[col].values.astype(float).reshape(-1, 1))\n",
    "        elif col in ['day_of_month']:\n",
    "            scalers['numeric'][col] = MinMaxScaler().fit(only_train[col].values.astype(float).reshape(-1, 1))\n",
    "        else:\n",
    "            scalers['numeric'][col] = QuantileTransformer(n_quantiles=256).fit(\n",
    "                only_train[col].values.astype(float).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8fd5c",
   "metadata": {},
   "source": [
    "### Transform by Applying Scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf1be26",
   "metadata": {},
   "source": [
    "After fitting the scalers and the encoders we apply them in order to the transform the entire dataset. Note that some categories appearing in the complete dataset, might not be \"familiar\" to the associated label encoder. Such keys will be mapped to a new ordinal label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(feature_cols):\n",
    "\n",
    "    if col in categorical_attrs:\n",
    "        le = scalers['categorical'][col]\n",
    "        # handle cases with unseen keys\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        data_df[col] = data_df[col].apply(lambda x: le_dict.get(x, max(le.transform(le.classes_)) + 1))\n",
    "        data_df[col] = data_df[col].astype(np.int32)\n",
    "    else:\n",
    "        data_df[col] = scalers['numeric'][col].transform(data_df[col].values.reshape(-1, 1)).squeeze()\n",
    "        data_df[col] = data_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75f545",
   "metadata": {},
   "source": [
    "After performing the transformations above, and in order to avoid null records on the target variable, we impute target signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['log_sales'].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278b38f",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d5e16",
   "metadata": {},
   "source": [
    "This stage deals with generating distinct subsets of the data for training, validation and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = {'train': dict(), 'validation': dict(), 'test': dict()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91451977",
   "metadata": {},
   "source": [
    "For each combination of (store,item), first we will slice the data into the training periods, as well as the validation and testing period. This primary slicing will be determined according to the argument we set in the beginning: ```validation_bound```, ```history_len```, and ```future_len```.<br/>\n",
    "\n",
    "Then we'll slide over each slice, with offset steps dictated by ```samp_interval```. for each slide (if the resulting sub-slicing results with sufficient time steps: ```history_len + future_len```), we split the feature set according to the data related keys:\n",
    "* ```static_feats_numeric```\n",
    "* ```static_feats_categorical```\n",
    "* ```historical_ts_numeric```\n",
    "* ```historical_ts_categorical```\n",
    "* ```future_ts_numeric```\n",
    "* ```future_ts_categorical```\n",
    "* ```target```\n",
    "\n",
    "where the temporal elements in this division, for each time-series, are represented as 2D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination_id, combination_seq in tqdm(data_df.groupby('combination_id')):\n",
    "\n",
    "    # take the complete sequence associated with this combination and break it into the relevant periods\n",
    "    train_subset = combination_seq.loc[combination_seq['date'] < validation_bound]\n",
    "    num_train_records = len(train_subset)\n",
    "    validation_subset_len = num_train_records + future_len\n",
    "    validation_subset = combination_seq.iloc[num_train_records - history_len: validation_subset_len]\n",
    "    test_subset = combination_seq.iloc[validation_subset_len - history_len:]\n",
    "\n",
    "    subsets_dict = {'train': train_subset,\n",
    "                    'validation': validation_subset,\n",
    "                    'test': test_subset}\n",
    "\n",
    "    # for the specific combination we're processing in the current iteration,\n",
    "    # we'd like to go over each subset separately\n",
    "    for subset_key, subset_data in subsets_dict.items():\n",
    "        # sliding window, according to samp_interval skips between adjacent windows\n",
    "        for i in range(0, len(subset_data), samp_interval):\n",
    "            # slice includes history period and horizons period\n",
    "            slc = subset_data.iloc[i: i + history_len + future_len]\n",
    "\n",
    "            if len(slc) < (history_len + future_len):\n",
    "                # skip edge cases, where not enough steps are included\n",
    "                continue\n",
    "\n",
    "            # meta\n",
    "            data_sets[subset_key].setdefault('time_index', []).append(slc.iloc[history_len - 1]['date'])\n",
    "            data_sets[subset_key].setdefault('combination_id', []).append(combination_id)\n",
    "\n",
    "            # static attributes\n",
    "            data_sets[subset_key].setdefault('static_feats_numeric', []).append(\n",
    "                slc.iloc[0][feature_map['static_feats_numeric']].values.astype(np.float32))\n",
    "            data_sets[subset_key].setdefault('static_feats_categorical', []).append(\n",
    "                slc.iloc[0][feature_map['static_feats_categorical']].values.astype(np.int32))\n",
    "\n",
    "            # historical\n",
    "            data_sets[subset_key].setdefault('historical_ts_numeric', []).append(\n",
    "                slc.iloc[:history_len][feature_map['historical_ts_numeric']].values.astype(np.float32).reshape(\n",
    "                    history_len, -1))\n",
    "            data_sets[subset_key].setdefault('historical_ts_categorical', []).append(\n",
    "                slc.iloc[:history_len][feature_map['historical_ts_categorical']].values.astype(np.int32).reshape(\n",
    "                    history_len, -1))\n",
    "\n",
    "            # futuristic (known)\n",
    "            data_sets[subset_key].setdefault('future_ts_numeric', []).append(\n",
    "                slc.iloc[history_len:][feature_map['future_ts_numeric']].values.astype(np.float32).reshape(future_len,\n",
    "                                                                                                           -1))\n",
    "            data_sets[subset_key].setdefault('future_ts_categorical', []).append(\n",
    "                slc.iloc[history_len:][feature_map['future_ts_categorical']].values.astype(np.int32).reshape(future_len,\n",
    "                                                                                                             -1))\n",
    "\n",
    "            # target\n",
    "            data_sets[subset_key].setdefault('target', []).append(\n",
    "                slc.iloc[history_len:]['log_sales'].values.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0caa78",
   "metadata": {},
   "source": [
    "After generating the above mentioned sets, we'll want to concatenate them into arrays for easier processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each set\n",
    "for set_key in list(data_sets.keys()):\n",
    "    # for each component in the set\n",
    "    for arr_key in list(data_sets[set_key].keys()):\n",
    "        # list of arrays will be concatenated\n",
    "        if isinstance(data_sets[set_key][arr_key], np.ndarray):\n",
    "            data_sets[set_key][arr_key] = np.stack(data_sets[set_key][arr_key], axis=0)\n",
    "        # lists will be transformed into arrays\n",
    "        else:\n",
    "            data_sets[set_key][arr_key] = np.array(data_sets[set_key][arr_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c920fd2",
   "metadata": {},
   "source": [
    "## Export processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45712cc1",
   "metadata": {},
   "source": [
    "Last step to perform is save this processed data to disk, together with the relevant meta data we'll need for building the model and analyzing its outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46202b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_path, 'data.pickle'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'data_sets': data_sets,\n",
    "        'feature_map': feature_map,\n",
    "        'scalers': scalers,\n",
    "        'categorical_cardinalities': categorical_cardinalities\n",
    "    }, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45948ed1-ebc2-4959-992d-1a9f5e9e605b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And that's it! We're done with the generation of the suitable dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tft",
   "language": "python",
   "name": "tft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
